{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "All_tiles_FCN_Baseline_Model_MSD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUSqvHTXaFeV"
      },
      "source": [
        "# All tiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZU_d5Ik9NaZ"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkLLIDs2p-_t"
      },
      "source": [
        "Installing ncecessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJoo92bydg0Z",
        "outputId": "01ce26e8-93e5-4627-c26b-016d3d5673bf"
      },
      "source": [
        "!pip install fiona\r\n",
        "!pip install rasterio\r\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fiona\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/94/4910fd55246c1d963727b03885ead6ef1cd3748a465f7b0239ab25dfc9a3/Fiona-1.8.18-cp36-cp36m-manylinux1_x86_64.whl (14.8MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8MB 305kB/s \n",
            "\u001b[?25hCollecting cligj>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/42/1e/947eadf10d6804bf276eb8a038bd5307996dceaaa41cfd21b7a15ec62f5d/cligj-0.7.1-py3-none-any.whl\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/da/824b92d9942f4e472702488857914bdd50f73021efea15b4cad9aca8ecef/click_plugins-1.1.1-py2.py3-none-any.whl\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona) (20.3.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona) (7.1.2)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from fiona) (2020.12.5)\n",
            "Installing collected packages: cligj, click-plugins, munch, fiona\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.1 fiona-1.8.18 munch-2.5.0\n",
            "Collecting rasterio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/1a/51baddc8581ead98fcef591624b4b2521b581943a9178912a2ac576e0235/rasterio-1.1.8-1-cp36-cp36m-manylinux1_x86_64.whl (18.3MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3MB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.19.5)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from rasterio) (7.1.2)\n",
            "Collecting affine\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/a6/1a39a1ede71210e3ddaf623982b06ecfc5c5c03741ae659073159184cd3e/affine-2.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.6/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from rasterio) (0.7.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from rasterio) (20.3.0)\n",
            "Collecting snuggs>=1.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/0e/d27d6e806d6c0d1a2cfdc5d1f088e42339a0a54a09c3343f7f81ec8947ea/snuggs-1.4.7-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n",
            "Installing collected packages: affine, snuggs, rasterio\n",
            "Successfully installed affine-2.3.0 rasterio-1.1.8 snuggs-1.4.7\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "\u001b[31m  ERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mh5KpzjdzyP"
      },
      "source": [
        "import rasterio \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import fiona\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "torch.backends.cudnn.benchmark = True\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "from tqdm import tqdm\r\n",
        "from torchvision.transforms import transforms\r\n",
        "import os\r\n",
        "import sys\r\n",
        "from rasterio.windows import Window\r\n",
        "from rasterio.errors import RasterioError, RasterioIOError\r\n",
        "from torchvision import transforms\r\n",
        "from torch.utils.data.dataset import IterableDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi5gUyrCeQqu",
        "outputId": "4793bfa4-7d63-4c3f-bf75-61833dbd9745"
      },
      "source": [
        "#Mount the drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGJ9wgEI9VJd"
      },
      "source": [
        "### Split data in to train and validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49VsQk32d3_b"
      },
      "source": [
        "Using NLCD as labels, train with both years at the same time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V23OvgBZd0bF"
      },
      "source": [
        "image_fns = []\r\n",
        "label_fns = []\r\n",
        "groups = []\r\n",
        "with fiona.open(\"/content/drive/MyDrive/Data/DFC_2021/dfc2021_index.geojson\") as f:\r\n",
        "    for row in f:\r\n",
        "        properties = row[\"properties\"]\r\n",
        "        image_fns.append(properties[\"naip-2013\"])\r\n",
        "        label_fns.append(properties[\"nlcd-2013\"])\r\n",
        "        groups.append(0)\r\n",
        "        \r\n",
        "        image_fns.append(properties[\"naip-2017\"])\r\n",
        "        label_fns.append(properties[\"nlcd-2016\"])\r\n",
        "        groups.append(1)\r\n",
        "\r\n",
        "df = pd.DataFrame.from_dict({\r\n",
        "    \"image_fn\": image_fns,\r\n",
        "    \"label_fn\": label_fns,\r\n",
        "    \"group\": groups\r\n",
        "})\r\n",
        "\r\n",
        "df.to_csv(\"/content/drive/MyDrive/Data/DFC_2021/training_set_naip_nlcd_both.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwVF0zIgiA3k"
      },
      "source": [
        "Using NLCD as labels, train with a single year at a time\r\n",
        "\r\n",
        "2013 NAIP, 2013 NLCD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVFJP-_3hy_I"
      },
      "source": [
        "image_fns = []\r\n",
        "label_fns = []\r\n",
        "groups = []\r\n",
        "with fiona.open(\"/content/drive/MyDrive/Data/DFC_2021/dfc2021_index.geojson\") as f:\r\n",
        "    for row in f:\r\n",
        "        properties = row[\"properties\"]\r\n",
        "        image_fns.append(properties[\"naip-2013\"])\r\n",
        "        label_fns.append(properties[\"nlcd-2013\"])\r\n",
        "        groups.append(0)\r\n",
        "\r\n",
        "df = pd.DataFrame.from_dict({\r\n",
        "    \"image_fn\": image_fns,\r\n",
        "    \"label_fn\": label_fns,\r\n",
        "    \"group\": groups\r\n",
        "})\r\n",
        "\r\n",
        "df.to_csv(\"/content/drive/MyDrive/Data/DFC_2021/training_set_naip_nlcd_2013.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APNkV6sbim-T"
      },
      "source": [
        "2017 NAIP, 2017 NLCD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMLK14XUidwx"
      },
      "source": [
        "image_fns = []\r\n",
        "label_fns = []\r\n",
        "groups = []\r\n",
        "with fiona.open(\"/content/drive/MyDrive/Data/DFC_2021/dfc2021_index.geojson\") as f:\r\n",
        "    for row in f:\r\n",
        "        properties = row[\"properties\"]\r\n",
        "        image_fns.append(properties[\"naip-2017\"])\r\n",
        "        label_fns.append(properties[\"nlcd-2016\"])\r\n",
        "        groups.append(1)\r\n",
        "\r\n",
        "df = pd.DataFrame.from_dict({\r\n",
        "    \"image_fn\": image_fns,\r\n",
        "    \"label_fn\": label_fns,\r\n",
        "    \"group\": groups\r\n",
        "})\r\n",
        "\r\n",
        "df.to_csv(\"/content/drive/MyDrive/Data/DFC_2021/training_set_naip_nlcd_2017.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LElThq7wkoJU"
      },
      "source": [
        "# Color maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTgnSn0XABwx"
      },
      "source": [
        "NUM_WORKERS = 0\r\n",
        "NUM_CHIPS_PER_TILE = 100\r\n",
        "CHIP_SIZE = 32\r\n",
        "\r\n",
        "NAIP_2013_MEANS = np.array([117.00, 130.75, 122.50, 159.30])\r\n",
        "NAIP_2013_STDS = np.array([38.16, 36.68, 24.30, 66.22])\r\n",
        "NAIP_2017_MEANS = np.array([72.84,  86.83, 76.78, 130.82])\r\n",
        "NAIP_2017_STDS = np.array([41.78, 34.66, 28.76, 58.95])\r\n",
        "NLCD_CLASSES = [ 0, 11, 12, 21, 22, 23, 24, 31, 41, 42, 43, 52, 71, 81, 82, 90, 95] # 16 classes + 1 nodata class (\"0\"). Note that \"12\" is \"Perennial Ice/Snow\" and is not present in Maryland.\r\n",
        "\r\n",
        "NLCD_CLASS_COLORMAP = { # Copied from the emebedded color table in the NLCD data files\r\n",
        "    0:  (0, 0, 0, 255),\r\n",
        "    11: (70, 107, 159, 255),\r\n",
        "    12: (209, 222, 248, 255),\r\n",
        "    21: (222, 197, 197, 255),\r\n",
        "    22: (217, 146, 130, 255),\r\n",
        "    23: (235, 0, 0, 255),\r\n",
        "    24: (171, 0, 0, 255),\r\n",
        "    31: (179, 172, 159, 255),\r\n",
        "    41: (104, 171, 95, 255),\r\n",
        "    42: (28, 95, 44, 255),\r\n",
        "    43: (181, 197, 143, 255),\r\n",
        "    52: (204, 184, 121, 255),\r\n",
        "    71: (223, 223, 194, 255),\r\n",
        "    81: (220, 217, 57, 255),\r\n",
        "    82: (171, 108, 40, 255),\r\n",
        "    90: (184, 217, 235, 255),\r\n",
        "    95: (108, 159, 184, 255)\r\n",
        "}\r\n",
        "\r\n",
        "NLCD_IDX_COLORMAP = {\r\n",
        "    idx: NLCD_CLASS_COLORMAP[c]\r\n",
        "    for idx, c in enumerate(NLCD_CLASSES)\r\n",
        "}\r\n",
        "\r\n",
        "def get_nlcd_class_to_idx_map():\r\n",
        "    nlcd_label_to_idx_map = []\r\n",
        "    idx = 0\r\n",
        "    for i in range(NLCD_CLASSES[-1]+1):\r\n",
        "        if i in NLCD_CLASSES:\r\n",
        "            nlcd_label_to_idx_map.append(idx)\r\n",
        "            idx += 1\r\n",
        "        else:\r\n",
        "            nlcd_label_to_idx_map.append(0)\r\n",
        "    nlcd_label_to_idx_map = np.array(nlcd_label_to_idx_map).astype(np.int64)\r\n",
        "    return nlcd_label_to_idx_map\r\n",
        "\r\n",
        "NLCD_CLASS_TO_IDX_MAP = get_nlcd_class_to_idx_map() # I do this computation on import for illustration (this could instead be a length 96 vector that is hardcoded here)\r\n",
        "\r\n",
        "\r\n",
        "NLCD_IDX_TO_REDUCED_LC_MAP = np.array([\r\n",
        "    4,#  0 No data 0\r\n",
        "    0,#  1 Open Water\r\n",
        "    4,#  2 Ice/Snow\r\n",
        "    2,#  3 Developed Open Space\r\n",
        "    3,#  4 Developed Low Intensity\r\n",
        "    3,#  5 Developed Medium Intensity\r\n",
        "    3,#  6 Developed High Intensity\r\n",
        "    3,#  7 Barren Land\r\n",
        "    1,#  8 Deciduous Forest\r\n",
        "    1,#  9 Evergreen Forest\r\n",
        "    1,# 10 Mixed Forest\r\n",
        "    1,# 11 Shrub/Scrub\r\n",
        "    2,# 12 Grassland/Herbaceous\r\n",
        "    2,# 13 Pasture/Hay\r\n",
        "    2,# 14 Cultivated Crops\r\n",
        "    1,# 15 Woody Wetlands\r\n",
        "    1,# 16 Emergent Herbaceious Wetlands\r\n",
        "])\r\n",
        "\r\n",
        "NLCD_IDX_TO_REDUCED_LC_ACCUMULATOR = np.array([\r\n",
        "    [0,0,0,0,1],#  0 No data 0\r\n",
        "    [1,0,0,0,0],#  1 Open Water\r\n",
        "    [0,0,0,0,1],#  2 Ice/Snow\r\n",
        "    [0,0,0,0,0],#  3 Developed Open Space\r\n",
        "    [0,0,0,0,0],#  4 Developed Low Intensity\r\n",
        "    [0,0,0,1,0],#  5 Developed Medium Intensity\r\n",
        "    [0,0,0,1,0],#  6 Developed High Intensity\r\n",
        "    [0,0,0,0,0],#  7 Barren Land\r\n",
        "    [0,1,0,0,0],#  8 Deciduous Forest\r\n",
        "    [0,1,0,0,0],#  9 Evergreen Forest\r\n",
        "    [0,1,0,0,0],# 10 Mixed Forest\r\n",
        "    [0,1,0,0,0],# 11 Shrub/Scrub\r\n",
        "    [0,0,1,0,0],# 12 Grassland/Herbaceous\r\n",
        "    [0,0,1,0,0],# 13 Pasture/Hay\r\n",
        "    [0,0,1,0,0],# 14 Cultivated Crops\r\n",
        "    [0,1,0,0,0],# 15 Woody Wetlands\r\n",
        "    [0,1,0,0,0],# 16 Emergent Herbaceious Wetlands\r\n",
        "])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNT1yAiUk4cK"
      },
      "source": [
        "# Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlZBnST5lALv"
      },
      "source": [
        "def image_transforms(img, group):\r\n",
        "    if group == 0:\r\n",
        "        img = (img - NAIP_2013_MEANS) / NAIP_2013_STDS\r\n",
        "    elif group == 1:\r\n",
        "        img = (img - NAIP_2017_MEANS) / NAIP_2017_STDS\r\n",
        "    else:\r\n",
        "        raise ValueError(\"group not recognized\")\r\n",
        "    img = np.rollaxis(img, 2, 0).astype(np.float32)\r\n",
        "    img = torch.from_numpy(img)\r\n",
        "    return img\r\n",
        "\r\n",
        "def label_transforms(labels, group):\r\n",
        "    labels = NLCD_CLASS_TO_IDX_MAP[labels]\r\n",
        "    labels = torch.from_numpy(labels)\r\n",
        "    return labels\r\n",
        "\r\n",
        "def nodata_check(img, labels):\r\n",
        "    return np.any(labels == 0) or np.any(np.sum(img == 0, axis=2) == 4)\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "    print(\"Starting DFC2021 baseline training script at %s\" % (str(datetime.datetime.now())))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsctQGos9rN9"
      },
      "source": [
        "# Tiles Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KQVzSuWJavX"
      },
      "source": [
        "class StreamingGeospatialDataset(IterableDataset):\r\n",
        "    \r\n",
        "    def __init__(self, imagery_fns, label_fns=None, groups=None, chip_size=256, num_chips_per_tile=200, windowed_sampling=False, image_transform=None, label_transform=None, nodata_check=None, verbose=False):\r\n",
        "        \"\"\"A torch Dataset for randomly sampling chips from a list of tiles. When used in conjunction with a DataLoader that has `num_workers>1` this Dataset will assign each worker to sample chips from disjoint sets of tiles.\r\n",
        "        Args:\r\n",
        "            imagery_fns: A list of filenames (or URLS -- anything that `rasterio.open()` can read) pointing to imagery tiles.\r\n",
        "            label_fns: A list of filenames of the same size as `imagery_fns` pointing to label mask tiles or `None` if the Dataset should operate in \"imagery only mode\". Note that we expect `imagery_fns[i]` and `label_fns[i]` to have the same dimension and coordinate system.\r\n",
        "            groups: Optional: A list of integers of the same size as `imagery_fns` that gives the \"group\" membership of each tile. This can be used to normalize imagery from different groups differently.\r\n",
        "            chip_size: Desired size of chips (in pixels).\r\n",
        "            num_chips_per_tile: Desired number of chips to sample for each tile.\r\n",
        "            windowed_sampling: Flag indicating whether we should sample each chip with a read using `rasterio.windows.Window` or whether we should read the whole tile into memory, then sample chips.\r\n",
        "            image_transform: A function to apply to each image chip object. If this is `None`, then the only transformation applied to the loaded imagery will be to convert it to a `torch.Tensor`. If this is not `None`, then the function should return a `Torch.tensor`. Further, if `groups` is not `None` then the transform function should expect the imagery as the first argument and the group as the second argument.\r\n",
        "            label_transform: Similar to image_transform, but applied to label chips.\r\n",
        "            nodata_check: A method that will check an `(image_chip)` or `(image_chip, label_chip)` (if `label_fns` are provided) and return whether or not the chip should be skipped. This can be used, for example, to skip chips that contain nodata values.\r\n",
        "            verbose: If `False` we will be quiet.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if label_fns is None:\r\n",
        "            self.fns = imagery_fns\r\n",
        "            self.use_labels = False\r\n",
        "        else:\r\n",
        "            self.fns = list(zip(imagery_fns, label_fns)) \r\n",
        "            self.use_labels = True\r\n",
        "\r\n",
        "        self.groups = groups\r\n",
        "\r\n",
        "        self.chip_size = chip_size\r\n",
        "        self.num_chips_per_tile = num_chips_per_tile\r\n",
        "        self.windowed_sampling = windowed_sampling\r\n",
        "\r\n",
        "        self.image_transform = image_transform\r\n",
        "        self.label_transform = label_transform\r\n",
        "        self.nodata_check = nodata_check\r\n",
        "\r\n",
        "        self.verbose = verbose\r\n",
        "\r\n",
        "        if self.verbose:\r\n",
        "            print(\"Constructed StreamingGeospatialDataset\")\r\n",
        "\r\n",
        "    def stream_tile_fns(self):\r\n",
        "        worker_info = torch.utils.data.get_worker_info()\r\n",
        "        if worker_info is None: # In this case we are not loading through a DataLoader with multiple workers\r\n",
        "            worker_id = 0\r\n",
        "            num_workers = 1\r\n",
        "        else:\r\n",
        "            worker_id = worker_info.id\r\n",
        "            num_workers = worker_info.num_workers\r\n",
        "\r\n",
        "        # We only want to shuffle the order we traverse the files if we are the first worker (else, every worker will shuffle the files...)\r\n",
        "        if worker_id == 0:\r\n",
        "            np.random.shuffle(self.fns) # in place\r\n",
        "        # NOTE: A warning, when different workers are created they will all have the same numpy random seed, however will have different torch random seeds. If you want to use numpy random functions, seed appropriately.\r\n",
        "        #seed = torch.randint(low=0,high=2**32-1,size=(1,)).item()\r\n",
        "        #np.random.seed(seed) # when different workers spawn, they have the same numpy random seed...\r\n",
        "\r\n",
        "        if self.verbose:\r\n",
        "            print(\"Creating a filename stream for worker %d\" % (worker_id))\r\n",
        "\r\n",
        "        # This logic splits up the list of filenames into `num_workers` chunks. Each worker will recieve ceil(num_filenames / num_workers) filenames to generate chips from. If the number of workers doesn't divide the number of filenames evenly then the last worker will have fewer filenames.\r\n",
        "        N = len(self.fns)\r\n",
        "        num_files_per_worker = int(np.ceil(N / num_workers))\r\n",
        "        lower_idx = worker_id * num_files_per_worker\r\n",
        "        upper_idx = min(N, (worker_id+1) * num_files_per_worker)\r\n",
        "        for idx in range(lower_idx, upper_idx):\r\n",
        "\r\n",
        "            label_fn = None\r\n",
        "            if self.use_labels:\r\n",
        "                img_fn, label_fn = self.fns[idx]\r\n",
        "            else:\r\n",
        "                img_fn = self.fns[idx]\r\n",
        "\r\n",
        "            if self.groups is not None:\r\n",
        "                group = self.groups[idx]\r\n",
        "            else:\r\n",
        "                group = None\r\n",
        "\r\n",
        "            if self.verbose:\r\n",
        "                print(\"Worker %d, yielding file %d\" % (worker_id, idx))\r\n",
        "\r\n",
        "            yield (img_fn, label_fn, group)\r\n",
        "\r\n",
        "    def stream_chips(self):\r\n",
        "        for img_fn, label_fn, group in self.stream_tile_fns():\r\n",
        "            num_skipped_chips = 0\r\n",
        "\r\n",
        "            # Open file pointers\r\n",
        "            img_fp = rasterio.open(img_fn, \"r\")\r\n",
        "            label_fp = rasterio.open(label_fn, \"r\") if self.use_labels else None\r\n",
        "\r\n",
        "            height, width = img_fp.shape\r\n",
        "            if self.use_labels: # garuntee that our label mask has the same dimensions as our imagery\r\n",
        "                t_height, t_width = label_fp.shape\r\n",
        "                assert height == t_height and width == t_width\r\n",
        "\r\n",
        "\r\n",
        "            # If we aren't in windowed sampling mode then we should read the entire tile up front\r\n",
        "            img_data = None\r\n",
        "            label_data = None\r\n",
        "            try:\r\n",
        "                if not self.windowed_sampling:\r\n",
        "                    img_data = np.rollaxis(img_fp.read(), 0, 3)\r\n",
        "                    if self.use_labels:\r\n",
        "                        label_data = label_fp.read().squeeze() # assume the label geotiff has a single channel\r\n",
        "            except RasterioError as e:\r\n",
        "                print(\"WARNING: Error reading in entire file, skipping to the next file\")\r\n",
        "                continue\r\n",
        "\r\n",
        "            for i in range(self.num_chips_per_tile):\r\n",
        "                # Select the top left pixel of our chip randomly\r\n",
        "                x = np.random.randint(0, width-self.chip_size)\r\n",
        "                y = np.random.randint(0, height-self.chip_size)\r\n",
        "\r\n",
        "                # Read imagery / labels\r\n",
        "                img = None\r\n",
        "                labels = None\r\n",
        "                if self.windowed_sampling:\r\n",
        "                    try:\r\n",
        "                        img = np.rollaxis(img_fp.read(window=Window(x, y, self.chip_size, self.chip_size)), 0, 3)\r\n",
        "                        print(img.shape)\r\n",
        "                        if self.use_labels:\r\n",
        "                            labels = label_fp.read(window=Window(x, y, self.chip_size, self.chip_size)).squeeze()\r\n",
        "                    except RasterioError:\r\n",
        "                        print(\"WARNING: Error reading chip from file, skipping to the next chip\")\r\n",
        "                        continue\r\n",
        "                else:\r\n",
        "                    img = img_data[y:y+self.chip_size, x:x+self.chip_size, :]\r\n",
        "                    if self.use_labels:\r\n",
        "                        labels = label_data[y:y+self.chip_size, x:x+self.chip_size]\r\n",
        "\r\n",
        "                # Check for no data\r\n",
        "                if self.nodata_check is not None:\r\n",
        "                    if self.use_labels:\r\n",
        "                        skip_chip = self.nodata_check(img, labels)\r\n",
        "                    else:\r\n",
        "                        skip_chip = self.nodata_check(img)\r\n",
        "\r\n",
        "                    if skip_chip: # The current chip has been identified as invalid by the `nodata_check(...)` method\r\n",
        "                        num_skipped_chips += 1\r\n",
        "                        continue\r\n",
        "\r\n",
        "                # Transform the imagery\r\n",
        "                if self.image_transform is not None:\r\n",
        "                    if self.groups is None:\r\n",
        "                        img = self.image_transform(img)\r\n",
        "                    else:\r\n",
        "                        img = self.image_transform(img, group)\r\n",
        "                else:\r\n",
        "                    img = torch.from_numpy(img).squeeze()\r\n",
        "\r\n",
        "                # Transform the labels\r\n",
        "                if self.use_labels:\r\n",
        "                    if self.label_transform is not None:\r\n",
        "                        if self.groups is None:\r\n",
        "                            labels = self.label_transform(labels)\r\n",
        "                        else:\r\n",
        "                            labels = self.label_transform(labels, group)\r\n",
        "                    else:\r\n",
        "                        labels = torch.from_numpy(labels).squeeze()\r\n",
        "\r\n",
        "\r\n",
        "                # Note, that img should be a torch \"Double\" type (i.e. a np.float32) and labels should be a torch \"Long\" type (i.e. np.int64)\r\n",
        "                if self.use_labels:\r\n",
        "                    yield img, labels\r\n",
        "                else:\r\n",
        "                    yield img\r\n",
        "\r\n",
        "            # Close file pointers\r\n",
        "            img_fp.close()\r\n",
        "            if self.use_labels:\r\n",
        "                label_fp.close()\r\n",
        "\r\n",
        "            if num_skipped_chips>0 and self.verbose:\r\n",
        "                print(\"We skipped %d chips on %s\" % (img_fn))\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        if self.verbose:\r\n",
        "            print(\"Creating a new StreamingGeospatialDataset iterator\")\r\n",
        "        return iter(self.stream_chips())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKtBkjZOFJFB",
        "outputId": "0c2be0b0-3c95-4b10-ec39-6d349362a83f"
      },
      "source": [
        "#-------------------\r\n",
        "# Load input data\r\n",
        "#-------------------\r\n",
        "\r\n",
        "input_fn = '/content/drive/MyDrive/Data/DFC_2021/training_set_naip_nlcd_2013.csv'\r\n",
        "batch_size = 32\r\n",
        "CHIP_SIZE = 32\r\n",
        "NUM_CHIPS_PER_TILE = 200\r\n",
        "\r\n",
        "\r\n",
        "input_dataframe = pd.read_csv(input_fn)\r\n",
        "image_fns = input_dataframe[\"image_fn\"].values\r\n",
        "print(image_fns)\r\n",
        "label_fns = input_dataframe[\"label_fn\"].values\r\n",
        "groups = input_dataframe[\"group\"].values\r\n",
        "\r\n",
        "\r\n",
        "dataset = StreamingGeospatialDataset(\r\n",
        "imagery_fns=image_fns, label_fns=label_fns, groups=groups, chip_size=CHIP_SIZE, num_chips_per_tile=NUM_CHIPS_PER_TILE, windowed_sampling=False, verbose=False,\r\n",
        "image_transform=image_transforms, label_transform=label_transforms, nodata_check=nodata_check\r\n",
        "    )\r\n",
        "\r\n",
        "dataloader = torch.utils.data.DataLoader(\r\n",
        "dataset,\r\n",
        "batch_size= batch_size,\r\n",
        "num_workers=NUM_WORKERS,\r\n",
        "pin_memory=True,\r\n",
        "    )\r\n",
        "num_training_batches_per_epoch = int(len(image_fns) * NUM_CHIPS_PER_TILE / batch_size)\r\n",
        "print(\"We will be training with %d batches per epoch\" % (num_training_batches_per_epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['https://dfc2021.blob.core.windows.net/competition-data/naip-2013/546_naip-2013.tif'\n",
            " 'https://dfc2021.blob.core.windows.net/competition-data/naip-2013/597_naip-2013.tif'\n",
            " 'https://dfc2021.blob.core.windows.net/competition-data/naip-2013/596_naip-2013.tif'\n",
            " ...\n",
            " 'https://dfc2021.blob.core.windows.net/competition-data/naip-2013/549_naip-2013.tif'\n",
            " 'https://dfc2021.blob.core.windows.net/competition-data/naip-2013/548_naip-2013.tif'\n",
            " 'https://dfc2021.blob.core.windows.net/competition-data/naip-2013/547_naip-2013.tif']\n",
            "We will be training with 14062 batches per epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLcDc48biPqw",
        "outputId": "8649385b-4201-4b6b-ded2-cd6b0b41a495"
      },
      "source": [
        "dataiter = iter(dataloader)\r\n",
        "images, labels = dataiter.next()\r\n",
        "print(images.shape)\r\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 4, 32, 32])\n",
            "torch.Size([32, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnzr8qEjLm0H"
      },
      "source": [
        "## FCN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRGsaGD5b9qH"
      },
      "source": [
        "import functools\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class FCN(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, num_input_channels, num_output_classes, num_filters=64):\r\n",
        "        super(FCN,self).__init__()\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv2d(num_input_channels, num_filters, kernel_size=3, stride=1, padding=1)\r\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters,        kernel_size=3, stride=1, padding=1)\r\n",
        "        self.conv3 = nn.Conv2d(num_filters, num_filters,        kernel_size=3, stride=1, padding=1)\r\n",
        "        self.conv4 = nn.Conv2d(num_filters, num_filters,        kernel_size=3, stride=1, padding=1)\r\n",
        "        self.conv5 = nn.Conv2d(num_filters, num_filters,        kernel_size=3, stride=1, padding=1)\r\n",
        "        self.last =  nn.Conv2d(num_filters, num_output_classes, kernel_size=1, stride=1, padding=0)\r\n",
        "\r\n",
        "    def forward(self,inputs):\r\n",
        "        x = F.relu(self.conv1(inputs))\r\n",
        "        x = F.relu(self.conv2(x))\r\n",
        "        x = F.relu(self.conv3(x))\r\n",
        "        x = F.relu(self.conv4(x))\r\n",
        "        x = F.relu(self.conv5(x))\r\n",
        "        x = self.last(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBNJwDsgcI1y",
        "outputId": "b05e2369-7ee3-4865-d279-7e573b802f7f"
      },
      "source": [
        "model = FCN(num_input_channels=4, num_output_classes=len(NLCD_CLASSES))\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FCN(\n",
            "  (conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (last): Conv2d(64, 17, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f59bMavwLyyu"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrCbUXeZSdkE"
      },
      "source": [
        "model = model\r\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.00001, amsgrad=True)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\")\r\n",
        "epochs = 50\r\n",
        "num_batches= 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ag3Wt-EFj-UN",
        "outputId": "de9d1809-de36-4d6d-84b4-8a721ad95df8"
      },
      "source": [
        "epoch = epochs\r\n",
        "for epoch in range(epoch):\r\n",
        "    losses = []\r\n",
        "    for batch_idx, (data, targets) in tqdm(enumerate(dataloader), total=num_batches, file=sys.stdout):\r\n",
        "    \r\n",
        "        #optimizer.zero_grad()\r\n",
        "        # get the inputs; data is a list of [inputs, labels]\r\n",
        "\r\n",
        "        output = model(data)\r\n",
        "        #print(output)\r\n",
        "        #print(targets)\r\n",
        "        print('the shape of output is',(output.shape))\r\n",
        "        print('the shape of x is',data.shape)\r\n",
        "        print('the shape of y is',targets.shape)\r\n",
        "        #loss = criterion(output, torch.max(y, 1)[1])\r\n",
        "        loss = criterion(output,targets)\r\n",
        "        losses.append(loss.item())\r\n",
        "        # zero the parameter gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "\r\n",
        "        print(\"Loss at {}th epoch: {}\".format(epoch,np.mean(losses)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/32 [00:00<?, ?it/s]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8004910945892334\n",
            "  3%|▎         | 1/32 [00:05<02:58,  5.77s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8025999069213867\n",
            "  6%|▋         | 2/32 [00:06<02:07,  4.24s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8038291931152344\n",
            "  9%|▉         | 3/32 [00:07<01:31,  3.17s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8026724457740784\n",
            " 12%|█▎        | 4/32 [00:07<01:07,  2.42s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8023110389709474\n",
            " 16%|█▌        | 5/32 [00:08<00:51,  1.90s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.802425503730774\n",
            " 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.804074696132115\n",
            " 22%|██▏       | 7/32 [00:13<01:00,  2.43s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8046542704105377\n",
            " 25%|██▌       | 8/32 [00:14<00:45,  1.91s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8051872783237033\n",
            " 28%|██▊       | 9/32 [00:15<00:35,  1.54s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.805341625213623\n",
            " 31%|███▏      | 10/32 [00:15<00:28,  1.28s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8057575009085913\n",
            " 34%|███▍      | 11/32 [00:16<00:23,  1.10s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8060284654299417\n",
            " 38%|███▊      | 12/32 [00:17<00:19,  1.02it/s]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.807445471103375\n",
            " 41%|████      | 13/32 [00:22<00:45,  2.38s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.808746780667986\n",
            " 44%|████▍     | 14/32 [00:23<00:33,  1.87s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.809235445658366\n",
            " 47%|████▋     | 15/32 [00:24<00:25,  1.51s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.810049206018448\n",
            " 50%|█████     | 16/32 [00:24<00:20,  1.26s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8108530886033\n",
            " 53%|█████▎    | 17/32 [00:25<00:16,  1.08s/it]the shape of output is torch.Size([32, 17, 32, 32])\n",
            "the shape of x is torch.Size([32, 4, 32, 32])\n",
            "the shape of y is torch.Size([32, 32, 32])\n",
            "Loss at 0th epoch: 2.8112272951338024\n",
            " 56%|█████▋    | 18/32 [00:26<00:13,  1.04it/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c492a9524aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-fe013c4b0df4>\u001b[0m in \u001b[0;36mstream_chips\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindowed_sampling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mimg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0mlabel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# assume the label geotiff has a single channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ65ssEyHhNu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}